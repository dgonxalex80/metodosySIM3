<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Métodos y Simulación Estadística" />


<title> Modelo de Regresión Lineal Simple</title>

<script src="site_libs/header-attrs-2.20/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"> </a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="recurso100.html">Análisis de correlación</a>
</li>
<li>
  <a href="recurso200.html">Regresión lineal simple</a>
</li>
<li>
  <a href="recurso300.html">Predicción</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore"><span style="color:#034a94"> <strong>Modelo
de Regresión Lineal Simple</strong></span></h1>
<h4 class="author">Métodos y Simulación Estadística</h4>

</div>


<p></br></br></p>
<div id="qué-es-el-análisis-de-regresión" class="section level1">
<h1><strong>¿Qué es el análisis de regresión?</strong></h1>
<p></br></p>
<p>El análisis de regresión es la búsqueda de una expresión matemçatica
que permita predecir los valores de una variable a través del
conocimiento de los valores de otra. La relación entre las dos variables
puede ser representada mediante la línea recta de mejor ajuste a los
datos. Esta línea se denomina recta de regresión o modelo de regresión
simple.</p>
<p>Para este análisis se dispone de una muestra de <span
class="math inline">\(n\)</span> pares de observaciones de una variable
<span class="math inline">\(Y\)</span>, que se llamaá variable
dependiente, la cual debe estar correlacionada con otra variable <span
class="math inline">\(X\)</span>, que se llamará variable independiente
<span class="math inline">\((x_{1},y_{1}), (x_{2},y_{2}),(x_{3},y_{3}),
...,(x_{n},y_{n})\)</span>.</p>
<p></br></br></p>
<div id="ejemplos" class="section level3">
<h3><strong>Ejemplos</strong>:</h3>
<p></br></p>
<ul>
<li>Si una empresa gasta 10 millones al año en publicidad, ¿Cuánto
podrían ser sus ingresos anuales por ventas?</li>
<li>Si una persona tiene una estatura de 170 centímetros, ¿Cuánto podría
ser su peso?</li>
<li>Si una estudiante le dedica 30 horas al estudio del próximo examen
de estadística, ¿Cuánto podría ser la calificación obtenida en dicha
prueba?</li>
<li>Para un futuro empresario que quiere vender 500 unidades de un
producto, ¿Cuánto podría ser el precio esperado de cada unidad?</li>
<li>¿Cuántas computadoras se venderían al mes?, si un vendedor realiza
50 llamadas a diferentes empresas.</li>
<li>Si el precio de un producto se aumenta un $2000, cuanto se espera se
reduzca su demanda?</li>
</ul>
<p></br></p>
<p>El establecimiento de la correlación entre las dos variables es
importante, pero esto se considera un primer paso para predecir una
variable a partir de la otra. Claro está, si sabe que la variable <span
class="math inline">\(X\)</span> está muy relacionada con <span
class="math inline">\(Y\)</span> , ello quiere decir que se puede
predecir <span class="math inline">\(Y\)</span> a partir de <span
class="math inline">\(X\)</span>. Se está ya en el terreno de la
predicción . (Evidentemente si, <span class="math inline">\(X\)</span>
no está correlacionada con <span class="math inline">\(Y\)</span>, la
variable <span class="math inline">\(X\)</span> no sirve como predictora
de <span class="math inline">\(Y\)</span>).</p>
<p></br></br></p>
</div>
<div id="la-ecuación-de-la-recta-de-regresión" class="section level2">
<h2><strong>La ecuación de la recta de regresión</strong></h2>
<p></br></p>
<p><span class="math display">\[Y = \beta_{0} + \beta_{1} X
+  \varepsilon \]</span> Donde :</p>
<ul>
<li><span class="math inline">\(Y\)</span> : variable dependiente o
respuesta</li>
<li><span class="math inline">\(X\)</span> : variable independiente o
regresora, la cual se supone conocida</li>
<li><span class="math inline">\(\varepsilon\)</span> : error, variable
aleatoria desconocida y que representa todas aquellas variables que no
estan en el modelo</li>
<li><span class="math inline">\(\beta_{0}\)</span> : coeficiente a
estimar, llamado intercepto</li>
<li><span class="math inline">\(\beta_{1}\)</span> : coeficiente a
estimar, llamado pendiente</li>
</ul>
<p><span class="math inline">\(\beta_{0}\)</span> y <span
class="math inline">\(\beta_{1}\)</span> conforman los parámetros a
estimar con conforman la recta:</p>
<p><span class="math display">\[\widehat{y_{i}} = \widehat{\beta}_{0} +
\widehat{\beta}_{1} x_i\]</span></p>
<p></br></br></p>
<p>El objetivo principal se centra en :</p>
<ul>
<li>predecir el valor de la variable dependiente <span
class="math inline">\(Y\)</span> a partir de un valor de la variable
independiente <span class="math inline">\(X\)</span></li>
<li>estimar el valor de la pendiente <span
class="math inline">\(\beta_{1}\)</span>, lo que permite valorar el
efecto generado sobre la variable dependiente <span
class="math inline">\(Y\)</span>, al realizar cambios en la variable
independiente <span class="math inline">\(X\)</span></li>
</ul>
<div id="ejemplo" class="section level3">
<h3><strong>Ejemplo</strong></h3>
<p>Se requiere estimar la cantidad de biomasa contenida en un bosque,
para lo cual se construye un modelo que permite estimar el valor de la
biomasa para un árbol utilizando para ello el diámetro del árbol</p>
<p><img src="recurso200_files/figure-html/unnamed-chunk-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Se observa en el diagrama que existe una relación positiva y fuerte
entre las dos variables</p>
<p></br></br></p>
</div>
</div>
<div id="estimación-de-los-parámetros" class="section level2">
<h2><strong>Estimación de los parámetros</strong></h2>
<p></br></p>
<div id="método-de-mínimos-cuadrados-ordinarios-mco"
class="section level3">
<h3><strong>Método de Mínimos Cuadrados Ordinarios (MCO)</strong></h3>
<p></br></p>
<p>Este método se base en la selección de los dos valores que conformen
la recta (intercepto y pendiente) que mejor se ajuste a los datos. Para
ello debe dar solución a un sistema de ecuaciones, denominadas
ecuaciones normales. A continuación se plantea como funciona el
métodos</p>
<!-- ```{r, echo = F, message = F, fig.align = 'center', out.width = '100%'} -->
<!-- data(biomasa)  -->
<!-- ggplot(biomasa, aes(x=altura , y=bio_total))+ -->
<!--   geom_point(size=2, colour=c3)+ -->
<!--   geom_smooth(method = "lm",se=FALSE)+ -->
<!--   labs(title = "", y= "biomasa total (tn) ", x= "altura (m) ") -->
<!--  -->
<!-- ``` -->
<p></br></p>
<p><img src="img/plotlm1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>La diferencia entre el valor de <span
class="math inline">\(y\)</span> asociado con un valor de <span
class="math inline">\(x_{0}\)</span> y el valor estimado por la recta de
regresión <span class="math inline">\(\widehat{y}_{0}\)</span> se
denomina <strong>resudual</strong> y constituye una muestra de la
variable aleatoria <span class="math inline">\(\varepsilon\)</span></p>
<p></br></p>
<p><span class="math display">\[e_i = y_{i}
-\widehat{y}_{i}\]</span></p>
<p></br></p>
<p>El método consiste en encontrar los valores de <span
class="math inline">\(\beta_{0}\)</span> y <span
class="math inline">\(\beta_{1}\)</span> que minimice la suma de los
cuadrados de los residuales</p>
<p></br></br></p>
<p><span class="math display">\[SCE = \sum_{i=1}^{n} e_{i}^2  =
\sum_{i=1}^{n} \big(y_{i} - \widehat{y}_{i}\big)^2  = \sum_{i=1}^{n}
\big( y_{i} - \widehat{\beta}_{0} - \beta_{1} x_{i}\big)^2\]</span>
</br></br></p>
<p>El objetivo del método es:</p>
<p><span class="math display">\[\min SCE = \dfrac{\partial SCE}{\partial
\beta_{0}} = 0\]</span></p>
<p><span class="math display">\[\min SCE = \dfrac{\partial SCE}{\partial
\beta_{1}} = 0\]</span> </br></br></p>
<p><span class="math display">\[\dfrac{\partial \hspace{.2cm}
\sum_{i=1}^{n} \big( y_{i} - \widehat{\beta}_{0} - \beta_{1}
x_{i}\big)^2}{\partial \beta_{0}} = -2 \sum(y_{i} - \beta_{0}- \beta_{1}
x_{i}) = 0\]</span></p>
<p></br></br></p>
<p><span class="math display">\[\dfrac{\partial \hspace{.2cm}
\sum_{i=1}^{n} \big( y_{i} - \widehat{\beta}_{0} - \beta_{1}
x_{i}\big)^2}{\partial \beta_{1}} = -2 \sum(y_{i} - \beta_{0}- \beta_{1}
x_{i}) x_{i} = 0\]</span> </br></br></p>
<p>Constituyendo un sistema de dos ecuaciones y dos incognitas,</p>
<p><span class="math display">\[\sum_{i=1}^{n} y_{i} = n
\widehat{\beta}_{0} + \widehat{\beta}_{1} \sum_{i=1}^{n}
x_{i}\]</span></p>
<p><span class="math display">\[\sum_{i=1}^{n} y_{i} x_{i}= n
\widehat{\beta}_{0} x_{i}+ \widehat{\beta}_{1} \sum_{i=1}^{n}
x_{i}^{2}\]</span> </br></br></p>
<p>Convirtiendose en el sistema :</p>
<p><span class="math display">\[
\begin{aligned}
n \widehat{\beta}_{0} + \widehat{\beta}_{1} \sum_{i=1}^{n} x_{i} &amp; =
&amp; \sum_{i=1}^{n} y_{i} \hspace{1cm}\text{(1)}\\
\widehat{\beta}_{0} \sum_{i=1}^{n} x_{i} + \widehat{\beta}_{1}
\sum_{i=1}^{n} x_{i}^{2} &amp; = &amp; \sum_{i=1}^{n} y_{i}
x_{i}  \hspace{1cm}\text{(2)}
\end{aligned}
\]</span></p>
<p></br></br></p>
<p>De la ecuación <span class="math inline">\((1)\)</span> se obtiene
:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat\beta_{0} &amp; = &amp;   \bar{y} - \widehat{\beta}_{1}
\bar{x}\\
\end{aligned}
\]</span> </br></br></p>
<p><span class="math display">\[
\begin{aligned}
\widehat\beta_{1} &amp; =  \dfrac{n \displaystyle\sum_{i=1}
x_{i}y_{i}  - \displaystyle\sum_{i=1}^{n}
x_{i}  \displaystyle\sum_{i=1}^{n} y_{i}}{n \displaystyle\sum_{i=1}^{n}
x_{i}^{2} - \bigg(\displaystyle\sum_{i=1}^{n}  x_{i}\bigg)^{2}} &amp;   
\end{aligned}
\]</span> </br></br></p>
</div>
<div id="estimación-de-máxima-verosimilitud" class="section level3">
<h3><strong>Estimación de Máxima Verosimilitud</strong></h3>
<p></br></p>
<p>Este método optimiza la probabilidad asociada a una distribución de
probabilidad, seleccionando los valores de <span
class="math inline">\(\beta_{0}\)</span> y <span
class="math inline">\(\beta_{1}\)</span> que hace tanga la mayor
probabilidad.</p>
<p>Parte del supuesto :</p>
<ul>
<li><p>Se tiene una muestra de tamaño <span
class="math inline">\(n\)</span></p></li>
<li><p>Los valores la variable <span
class="math inline">\(Y_{i}\)</span> se ajusta al modelo <span
class="math inline">\(Y_{i} = \beta_{0} + \beta_{1} X_{i} +
\varepsilon_{i}\)</span></p></li>
<li><p>Los valores de <span class="math inline">\(\varepsilon_{i} \sim
N( 0, \sigma^{2}\)</span>, e independientes</p></li>
<li><p><span class="math inline">\(Y \sim N(\beta_{0}+ \beta_{1}X_{i},
\sigma_{\varepsilon}^{2})\)</span></p></li>
<li><p>Para cada valor fijo de <span
class="math inline">\(x_{0}\)</span> : <span
class="math inline">\(E[Y|X_{i}=x_{0}] =
\beta_{0}+\beta_{1}x_{0}\)</span></p></li>
<li><p>La función de <span class="math inline">\(Y\)</span> cuando <span
class="math inline">\(X = x_{i}\)</span>:</p></li>
</ul>
<p></br></br></p>
<p><span class="math display">\[
f(y|_{X=x_{i}}) = \dfrac{1}{\sigma \sqrt{2 \pi}} \exp\Bigg\{
-\dfrac{1}{2\sigma_{\varepsilon}^2} \bigg(
y-\beta_{0}-\beta_{1}x_{i}\bigg)^{2}  \Bigg\}
\]</span> </br></br></p>
<p>Por tanto la función de distribución conjunta</p>
<p><span class="math display">\[
L(\beta_{0}, \beta_{1}, \sigma_{\varepsilon}) =  \Pi_{i=1}^{n} \Bigg(
\dfrac{1}{\sigma_{\varepsilon}\sqrt{2\pi}}  \Bigg)^n
\exp\Bigg\{-\dfrac{1}{2} \displaystyle\sum_{i=1}^{n}
\Bigg(\dfrac{y_{i}-(\beta_{0}-\beta_{1}x_{i})}{\sigma_{\varepsilon}}\Bigg)^{2}
\Bigg\}
\]</span></p>
<p></br></br></p>
<p>Para encontrar los valores de <span
class="math inline">\(\beta_{0}\)</span> y <span
class="math inline">\(\beta_{1}\)</span> que maximizan <span
class="math inline">\(L(\beta_{0}, \beta_{1},
\sigma_{\varepsilon})\)</span>, se utiliza la derivada parcial del <span
class="math inline">\(\log (L(\beta_{0}, \beta_{1},
\sigma_{\varepsilon}))\)</span></p>
<p></br></br></p>
<p><span class="math display">\[
\log \big(L(\beta_{0}, \beta_{1}, \sigma_{\varepsilon})\big) = -n
\hspace{.2cm} \log(\sigma_{\varepsilon}) - n \log(\sqrt{2\pi}) -
\dfrac{1}{2 \sigma_{\varepsilon}} \sum_{i=1}^{n} \big(y_{i}-
\beta_{0}-\beta_{1}x_{i}\big)^{2}
\]</span></p>
<p></br></br></p>
<p>Al derivar la función e igualarlo a cero tenemos</p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial}{\partial \beta_{0}} \log (L) &amp;=&amp;
\dfrac{1}{\sigma_{\varepsilon}} \sum_{i=1}^{n} (y_{i} - \beta_{0} -
\beta_{1} x_{i}) \hspace{.2cm} = 0  \hspace{.5cm}\text{(3)}
\end{aligned}
\]</span></p>
<p></br></br></p>
<p><span class="math display">\[
\begin{aligned}
\dfrac{\partial}{\partial \beta_{1}} \log (L) &amp;=&amp;
\dfrac{1}{\sigma_{\varepsilon}} \sum_{i=1}^{n} (y_{i} - \beta_{0} -
\beta_{1} x_{i}) x_{i} = 0  \hspace{.5cm}\text{(4)}
\end{aligned}
\]</span></p>
<p></br></br></p>
<p>Que conincide con las ecuaciones normales <span
class="math inline">\((1)\)</span> y <span
class="math inline">\((2)\)</span> .</p>
<p>En conclusión la solución obtenida por el método de mínimos cuadrados
y la obtenida por el método de máxima verosimilitud son iguales</p>
<p></br></br></p>
</div>
</div>
<div id="coeficiente-de-determinación" class="section level2">
<h2><strong>Coeficiente de determinación</strong></h2>
<p></br></p>
<p>Después de realizar la estimación de <span
class="math inline">\(MCO\)</span> se requiere un indicador que permita
medir el ajuste del modelo para con los datos. El coeficiente de
determinación <span class="math inline">\(R^2\)</span>. Este indicador
varia entre 0 y 1. Se desea que el modelo presenta un buen nivel de
ajuste. En caso de tener varios modelos, el que presente el mayor valor,
será el que mejor se ajuste a los datos.</p>
<p></br></p>
<p><span class="math display">\[R^{2} =
\dfrac{\displaystyle\sum_{i=1}^{n}
(\widehat{y}_{i}-\bar{y})^{2}}{\displaystyle\sum_{i=1}^{n}
(y_{i}-\bar{y})^{2}}\]</span></p>
<p>Para que <span class="math inline">\(R^2\)</span> sirva de indicador
de comparación entre modelos, los modelos deben tener la misma variable
dependiente. En estos casos se utiliza el <span
class="math inline">\(R^2_{ajd}\)</span></p>
<p></br></p>
<p><span class="math display">\[
R^2_{ajd} = 1 - \Bigg[\dfrac{n-1}{n-k-1} \Bigg] \Big(1-R^2 \Big)
\]</span> Ajustado por el número de variables independientes <span
class="math inline">\(k\)</span> y el tamaño de la muestra <span
class="math inline">\(n\)</span></p>
<p></br></p>
<p></br></br></p>
</div>
<div id="supuestos-del-modelo" class="section level2">
<h2><strong>Supuestos del modelo</strong></h2>
<p></br></p>
<p>El método MCO, es un método matemático que tiene solución única, sin
embargo, si se desea realizar inferencia estadística como intervalos de
confianzay pruebas de hipótesis es necesario validar los siguientes
supuestos:</p>
<p></br></p>
<ul>
<li><strong>Normalidad</strong>, los errores siguen una distribución
normal (<span class="math inline">\(\varepsilon_{i} \sim N(0,
\sigma^2)\)</span>)</li>
</ul>
<p></br></p>
<ul>
<li><strong>Homoscedasticidad</strong>, la varianza al rededor de la
linea de regresión, para cualquier valor constante (<span
class="math inline">\(V[\varepsilon] = \sigma^2\)</span>)</li>
</ul>
<p></br></p>
<ul>
<li><strong>Linealidad</strong>, la relación entre la variable
dependiente y las variables independientes y el error es lineal. Es
decir que las variables pueden tener cualquier forma pero los parametros
deben garantizar una relacion lineal (<span class="math inline">\(y_{i}=
\beta_{0}+ \beta_{1} x_{i} + \varepsilon_{i}\)</span>)</li>
</ul>
<p></br></p>
<ul>
<li><strong>No autocorrelación</strong>, los errores que corresponden a
diferentes individuos o difente tiempo deben ser independintes unos de
otros (<span class="math inline">\(Cov[\varepsilon_{i}, \varepsilon_{j}]
= 0\)</span>)</li>
</ul>
<p></br></p>
<ul>
<li><strong>Outliers</strong>, aunque no es un supuesto formal, se
espera que la data no contenga datos atípicos que generen sesgos en los
estimadores de los coeficientes</li>
</ul>
<p></br></br></p>
</div>
<div id="transformación-de-variables" class="section level2">
<h2><strong>Transformación de variables</strong></h2>
<p></br></p>
<p>En algunos casos el modelo lineal no es apropiado para ajutar los
datos y por tanto no cumple los supuestos sobre los errores. Es
necesario entonces probar otro tipo de modelo transformando las
variables.</p>
<p>Dentro de las transformaciones más empleadas estan:</p>
<center>
<table>
<colgroup>
<col width="7%" />
<col width="39%" />
<col width="14%" />
<col width="16%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Modelo</th>
<th align="center">Regresión</th>
<th align="center">Variable dependiente</th>
<th align="center">Variable independiente</th>
<th align="center">Interpretación de <span
class="math inline">\(\beta_{1}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>Lin-Lin</code></td>
<td align="center"><span class="math inline">\(Y_{i} = \beta_{0}+
\beta_{1} X_{i} + \varepsilon_{i}\)</span></td>
<td align="center"><span class="math inline">\(Y\)</span></td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center"><span class="math inline">\(\Delta Y = \beta_{1}
\Delta X\)</span></td>
</tr>
<tr class="even">
<td align="center"><code>Lin-Log</code></td>
<td align="center"><span class="math inline">\(Y_{i} = \beta_{0}+
\beta_{1} \log(X_{i}) + \varepsilon_{i}\)</span></td>
<td align="center"><span class="math inline">\(Y\)</span></td>
<td align="center"><span class="math inline">\(\log(X)\)</span></td>
<td align="center"><span class="math inline">\(\Delta Y =
\Bigg(\dfrac{\beta_{1}}{100}\Bigg) \% \Delta X\)</span></td>
</tr>
<tr class="odd">
<td align="center"><code>Log-Lin</code></td>
<td align="center"><span class="math inline">\(\log(Y_{i}) = \beta_{0}+
\beta_{1} X_{i} + \varepsilon_{i}\)</span></td>
<td align="center"><span class="math inline">\(\log(Y)\)</span></td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center"><span class="math inline">\(\%\Delta Y = (100
\beta_{1}) \Delta X\)</span></td>
</tr>
<tr class="even">
<td align="center"><code>Log-Log</code></td>
<td align="center"><span class="math inline">\(\log(Y_{i}) = \beta_{0}+
\beta_{1} \log(X_{i}) + \varepsilon_{i}\)</span></td>
<td align="center"><span class="math inline">\(\log(Y)\)</span></td>
<td align="center"><span class="math inline">\(\log(X)\)</span></td>
<td align="center"><span class="math inline">\(\%\Delta Y = \beta_{1}\%
\Delta X\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<!--```{r, echo=FALSE, out.width="100%", fig.align = "center"} -->
<!--knitr::include_graphics("img/interpreta.png") -->
<!--``` -->
</center>
<p></br></br></p>
</div>
<div id="código-r" class="section level2">
<h2><strong>Código R</strong></h2>
<p></br></p>
<p>A continuación se presenta el código en R para estimar el modelo y
validar los supuestos más importantes</p>
<p></br></p>
<div id="estimación-mco" class="section level3">
<h3><strong>Estimación MCO</strong></h3>
<pre class="r"><code>library(paqueteMET)
data(biomasa)
modelo=lm(bio_total ~ diametro, data=biomasa)
summary(modelo)</code></pre>
<p>Los resultados se presentan en cuartro partes :</p>
<p></br></br></p>
<p><strong>Formula del modelo</strong></p>
<p><span class="math display">\[\widehat{\text{bio_total}_{i}} =
\widehat{\beta}_{0} + \widehat{\beta}_{1}
\hspace{.2cm}\text{diametro}_{i}\]</span></p>
<pre>
Call:
lm(formula = log(bio_total) ~ diametro, data = biomasa)
</pre>
<p></br></br></p>
<p><strong>Estadística de los residuales</strong></p>
<pre>
Residuals:
    Min      1Q  Median      3Q     Max 
-6.3775 -2.6594  0.0237  1.8758 11.9876 
</pre>
<p></br></br></p>
<strong>Coeficientes estimados</strong>
<pre>
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -9.0203     1.4129  -6.384 7.86e-09 ***
diametro      5.1026     0.2508  20.346  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</pre>
<p></br></p>
<p><span class="math display">\[\widehat{\text{bio_total}_{i}} = -9.0203
+ 5.1026 \hspace{.2cm} \text{diametro}_{i}\]</span> Además se presentan
los resultados de las pruebas de hipótesis individuales sobre los
coeficientes:</p>
<!-- | Hipótesis             |  Estadístico de prueba        |   valor p            | -->
<!-- |:----------------------|:------------------------------|:---------------------| -->
<!-- $Ho: \beta_{0} = 0$     |   -6.384                      |  0.0000              | -->
<!-- $Ha: \beta_{0} \neq 0$  |                               |                      | -->
<!-- |                       |                               |                      | -->
<!-- |$Ho: \beta_{1} = 0$    |   20.346                      |  0.0000              | -->
<!-- |$Ha: \beta_{1} \neq 0$ |                               |                      | -->
<p><img src="img/pruebas1.png" width="50%" style="display: block; margin: auto;" /></p>
<p></br></br></p>
<p><strong>Indicadores de ajuste</strong></p>
<pre>
Residual standard error: 3.435 on 88 degrees of freedom
Multiple R-squared:  0.8247,    Adjusted R-squared:  0.8227 
F-statistic:   414 on 1 and 88 DF,  p-value: < 2.2e-16
</pre>
<p>Este resultado muestra el valor del coeficiente de determinación
(<span class="math inline">\(R^2\)</span>) que corresponde al porcentaje
de la variabilidad de <span class="math inline">\(Y\)</span> explicada
por el modelo. Para el ejemplo <span class="math inline">\(R^{2} =
0.8227\)</span>. indicando que el modelo explica un 82.27% de la
variación de <span class="math inline">\(Y\)</span>.</p>
<p></br></br></p>
</div>
<div id="validación-de-supuestos" class="section level3">
<h3><strong>Validación de supuestos</strong></h3>
<p></br></br></p>
</div>
<div id="normalidad" class="section level3">
<h3><strong>Normalidad</strong></h3>
<p></br></p>
<p><span class="math inline">\(H_o: \varepsilon \sim Normal\)</span></p>
<p><span class="math inline">\(H_a: \varepsilon \hspace{.2cm} \text{ no
} \sim Normal\)</span></p>
<p></br></br></p>
<div id="test-de-shapiro---wilk" class="section level4">
<h4><strong>Test de Shapiro - Wilk</strong></h4>
<p></br></p>
<pre class="r"><code>shapiro.test(modelo$residuals)</code></pre>
<pre><code>
    Shapiro-Wilk normality test

data:  modelo$residuals
W = 0.98394, p-value = 0.3338</code></pre>
<p></br></br></p>
</div>
<div id="test-de-jarque-bera" class="section level4">
<h4><strong>Test de Jarque-Bera</strong></h4>
<p></br></p>
<pre class="r"><code># install.packages(&quot;normtets&quot;)
 library(normtest)
normtest::jb.norm.test(modelo$residuals)</code></pre>
<pre><code>
    Jarque-Bera test for normality

data:  modelo$residuals
JB = 2.081, p-value = 0.253</code></pre>
<p></br></br></p>
</div>
<div id="test-de-anderson-darling" class="section level4">
<h4><strong>Test de Anderson-Darling</strong></h4>
<p></br></p>
<pre class="r"><code># install.packages(&quot;nortets&quot;)
 library(nortest)
ad.test(modelo$residuals)</code></pre>
<pre><code>
    Anderson-Darling normality test

data:  modelo$residuals
A = 0.29771, p-value = 0.5818</code></pre>
<p></br></br></p>
</div>
<div id="test-de-lilliefors-kolmogorov-smirnov" class="section level4">
<h4><strong>Test de Lilliefors (Kolmogorov-Smirnov)</strong></h4>
<p></br></p>
<pre class="r"><code>lillie.test(modelo$residuals)</code></pre>
<pre><code>
    Lilliefors (Kolmogorov-Smirnov) normality test

data:  modelo$residuals
D = 0.0623, p-value = 0.5281</code></pre>
<p></br></br></p>
</div>
</div>
<div id="varianza-contante" class="section level3">
<h3><strong>Varianza contante</strong></h3>
<p></br></p>
<p><span class="math inline">\(Ho : V[\varepsilon_{i}] =
\sigma^2\)</span></p>
<p><span class="math inline">\(Ha : V[\varepsilon_{i}] \neq
\sigma^2\)</span></p>
<p></br></br></p>
<div id="test-de-breusch-pagan" class="section level4">
<h4><strong>Test de Breusch-Pagan</strong></h4>
<p></br></p>
<pre class="r"><code>library(lmtest)
bptest(modelo)</code></pre>
<pre><code>
    studentized Breusch-Pagan test

data:  modelo
BP = 3.879, df = 1, p-value = 0.04889</code></pre>
<p></br></br></p>
</div>
<div id="test-de-goldfeld-quandt" class="section level4">
<h4><strong>Test de Goldfeld-Quandt</strong></h4>
<p></br></p>
<pre class="r"><code>library(lmtest)
gqtest(modelo)</code></pre>
<pre><code>
    Goldfeld-Quandt test

data:  modelo
GQ = 1.1538, df1 = 43, df2 = 43, p-value = 0.3206
alternative hypothesis: variance increases from segment 1 to 2</code></pre>
<p></br></br></p>
</div>
<div id="no-autocorrelación-de-errores" class="section level4">
<h4><strong>No autocorrelación de errores</strong></h4>
<p></br></p>
<p><span class="math inline">\(Ho : E[\varepsilon_{i}, \varepsilon_{j}]
= 0\)</span></p>
<p><span class="math inline">\(Ha : E[\varepsilon_{i}, \varepsilon_{j}]
\neq 0\)</span></p>
<p></br></p>
</div>
<div id="test-de-durbin-watson" class="section level4">
<h4><strong>Test de Durbin-Watson</strong></h4>
<p></br></p>
<pre class="r"><code>dwtest(modelo)</code></pre>
<pre><code>
    Durbin-Watson test

data:  modelo
DW = 0.67803, p-value = 1.716e-13
alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p></br></br></p>
</div>
<div id="transformación-de-variables-1" class="section level4">
<h4><strong>Transformación de variables</strong></h4>
<pre class="r"><code>library(paqueteMET)
data(&quot;biomasa&quot;)
modelo1=lm(bio_total ~ diametro, data=biomasa)           # Lin - Lin
modelo2=lm(bio_total ~ log(diametro), data=biomasa)      # Lin - Log
modelo3=lm(log(bio_total) ~ diametro, data=biomasa)      # Log - Lin
modelo4=lm(log(bio_total) ~ log(diametro), data=biomasa) # Log - Log</code></pre>
<pre class="r"><code>library(stargazer)
stargazer(modelo1, modelo2, modelo3, modelo4, type=&quot;text&quot;, df=FALSE)</code></pre>
<pre><code>
===============================================================
                                Dependent variable:            
                    -------------------------------------------
                          bio_total          log(bio_total)    
                       (1)        (2)        (3)        (4)    
---------------------------------------------------------------
diametro             5.103***              0.278***            
                     (0.251)               (0.011)             
                                                               
log(diametro)                  23.369***              1.344*** 
                                (1.564)               (0.058)  
                                                               
Constant            -9.020***  -19.909***  1.328***   0.618*** 
                     (1.413)    (2.629)    (0.060)    (0.098)  
                                                               
---------------------------------------------------------------
Observations            90         90         90         90    
R2                    0.825      0.717      0.887      0.858   
Adjusted R2           0.823      0.714      0.885      0.857   
Residual Std. Error   3.435      4.362      0.145      0.162   
F Statistic         413.961*** 223.224*** 687.562*** 532.232***
===============================================================
Note:                               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
